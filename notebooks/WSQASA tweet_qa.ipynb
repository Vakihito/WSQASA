{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEWlALV3-v-t",
        "outputId": "c3889d2c-0a8f-44aa-91a1-6ff041503a25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'WSQASA'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 72 (delta 24), reused 69 (delta 21), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (72/72), 24.62 MiB | 4.24 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# clone the repo\n",
        "!git clone https://github.com/<Anonymized>/WSQASA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "c14wuiHyyAhi"
      },
      "outputs": [],
      "source": [
        "### parameters for all pipes\n",
        "dataset_name = \"tweet_qa\"\n",
        "\n",
        "main_path = \"/content/WSQASA\"\n",
        "scripts_path = f\"{main_path}/scripts/pipe.zip\"\n",
        "input_file_issues_path = f'{main_path}/data/LID_all_issues.csv'\n",
        "\n",
        "dir_path = f'{main_path}/'\n",
        "data_dir_path = f'{dir_path}data/'\n",
        "model_dir_path = f'{dir_path}models/'\n",
        "input_file_train_data = f'{main_path}/data/train_data.pkl'\n",
        "random_seed = \"42\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xFAA139JCEK0"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "if not os.path.exists(main_path):\n",
        "  os.mkdir(main_path)\n",
        "\n",
        "if not os.path.exists(dir_path):\n",
        "  os.mkdir(dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RnZ4HNudyBf5"
      },
      "outputs": [],
      "source": [
        "### notebook 1 parameters ###\n",
        "min_thold = \"0.75\"\n",
        "min_sentiment_thold = \"-0.1\"\n",
        "search_model_name = \"all-MiniLM-L6-v2\"\n",
        "output_file_path_1 = f'{main_path}/data/syntatic_train_with_similarity.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "N_RcgWm4yCkC"
      },
      "outputs": [],
      "source": [
        "### notebook 2 parameters ###\n",
        "input_file_path_2 = f'{data_dir_path}{dataset_name}_validation_form.pkl'\n",
        "output_file_path_2 = f'{data_dir_path}{dataset_name}_test_with_similarity.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "o1wAPjSiyDyh"
      },
      "outputs": [],
      "source": [
        "### notebook 3 parameters ###\n",
        "domain = 'negative'\n",
        "input_file_path_3 = output_file_path_1\n",
        "output_model_path_3 = f'{model_dir_path}not_{dataset_name}_finetuned_{domain}_model'\n",
        "\n",
        "# sentiment models parameters #\n",
        "model_checkpoint = \"deepset/bert-base-cased-squad2\"\n",
        "batch_size = \"16\"\n",
        "\n",
        "# tokenizer\n",
        "max_length = \"512\"  # The maximum length of a feature (question and context)\n",
        "doc_stride = \"128\"  # The allowed overlap between two part of the context when splitting is performed.\n",
        "\n",
        "# hyper-parameters \n",
        "learning_rate = \"2e-5\"\n",
        "num_train_epochs = \"1\"\n",
        "weight_decay = \"0.01\"\n",
        "encoder_layers_to_freeze = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FmNAKJeToO7e"
      },
      "outputs": [],
      "source": [
        "### notebook 4 parameters ###\n",
        "input_file_path_4 = output_file_path_2\n",
        "input_model_path_4 = f'{model_dir_path}not_{dataset_name}_finetuned_{domain}_model'\n",
        "\n",
        "output_file_path_4 = f'{data_dir_path}{dataset_name}_validation_{domain}_with_finetuned_predictions.pkl'\n",
        "\n",
        "model_tokenizer = \"deepset/bert-base-cased-squad2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ylF3Wq_Ry7GV"
      },
      "outputs": [],
      "source": [
        "### notebook 4.1 parameters ###\n",
        "input_file_path_4_1 = output_file_path_2\n",
        "output_file_path_4_1 = f'{data_dir_path}{dataset_name}_validation_{domain}_with_not_finetuned_predictions.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EvC9gMB4zX36"
      },
      "outputs": [],
      "source": [
        "#### notebook 5 parameters ###\n",
        "input_file_path_not_finetuned_5 = output_file_path_4_1\n",
        "input_file_path_finetuned_5 = output_file_path_4\n",
        "\n",
        "output_file_path_5 = f'{data_dir_path}{dataset_name}_metrics.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rg0yoY--zz6B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# base variables \n",
        "os.environ['dataset_name'] = dataset_name\n",
        "os.environ['dir_path'] = dir_path\n",
        "os.environ['data_dir_path'] = data_dir_path\n",
        "os.environ['model_dir_path'] = model_dir_path\n",
        "os.environ['input_file_issues_path'] = input_file_issues_path\n",
        "os.environ['input_file_train_data'] = input_file_train_data\n",
        "os.environ['random_seed'] = random_seed\n",
        "\n",
        "## search variables - notebook 1,2\n",
        "os.environ['min_thold'] = min_thold\n",
        "os.environ['min_sentiment_thold'] = min_sentiment_thold\n",
        "os.environ['search_model_name'] = search_model_name\n",
        "os.environ['output_file_path_1'] = output_file_path_1\n",
        "os.environ['input_file_path_2'] = input_file_path_2\n",
        "os.environ['output_file_path_2'] = output_file_path_2\n",
        "\n",
        "## notebook 3 parameters train !&A models\n",
        "os.environ['domain'] = domain\n",
        "os.environ['input_file_path_3'] = input_file_path_3\n",
        "os.environ['output_model_path_3'] = output_model_path_3\n",
        "os.environ['model_checkpoint'] = model_checkpoint\n",
        "os.environ['batch_size'] = batch_size\n",
        "os.environ['max_length'] = max_length\n",
        "os.environ['doc_stride'] = doc_stride\n",
        "os.environ['learning_rate'] = learning_rate\n",
        "os.environ['num_train_epochs'] = num_train_epochs\n",
        "os.environ['weight_decay'] = weight_decay\n",
        "os.environ['encoder_layers_to_freeze'] = encoder_layers_to_freeze\n",
        "\n",
        "### notebook 4 parameters ###\n",
        "os.environ['input_file_path_4'] = input_file_path_4\n",
        "os.environ['input_model_path_4'] = input_model_path_4\n",
        "os.environ['output_file_path_4_1'] = output_file_path_4_1\n",
        "os.environ['output_file_path_4'] = output_file_path_4\n",
        "os.environ['input_file_path_4_1'] = input_file_path_4_1\n",
        "os.environ['model_tokenizer'] = model_tokenizer\n",
        "\n",
        "### notebooks 5 parameters ###\n",
        "os.environ['input_file_path_not_finetuned_5'] = input_file_path_not_finetuned_5\n",
        "os.environ['input_file_path_finetuned_5'] = input_file_path_finetuned_5\n",
        "os.environ['output_file_path_5'] = output_file_path_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mb_IN5cNrp0R"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "## notebook 1 dependencies ##\n",
        "!pip install faiss-gpu==1.7.2\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install vaderSentiment==3.3.2\n",
        "!pip install -U tensorflow==2.10 \n",
        "\n",
        "## notebook 1 dependencies ##\n",
        "!pip install transformers==4.22.2 datasets==2.5.1 \n",
        "\n",
        "## notebook 2 dependencies\n",
        "!pip install huggingface_hub==0.10.0\n",
        "\n",
        "## notebook 3 dependencies\n",
        "!pip install evaluate==0.2.2\n",
        "\n",
        "## notebook 5 dependencies\n",
        "!pip install tensorflow_probability==0.12.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hLWD1jR5_L6M"
      },
      "outputs": [],
      "source": [
        "!pip freeze > requerements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDEr6wi7BTJj",
        "outputId": "4e48e0a7-ce95-45ba-af6d-e54a9dd126ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/WSQASA/scripts/pipe.zip\n",
            "  inflating: 2_1_similarity_search_answer_test_df.py  \n",
            "  inflating: 2_similarity_search_answers_artificial_df.py  \n",
            "  inflating: 3_training_the_qa_model_on_syntatic_data.py  \n",
            "  inflating: 4_1_run_not_finetuned_model_over_dataset_with_specific_domain.py  \n",
            "  inflating: 4_run_model_over_dataset_with_specific_domain.py  \n",
            "  inflating: 5_compare_performance_of_models.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"$scripts_path\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qWCkYCdep-Fh"
      },
      "source": [
        "### 1. Check if the notebook train data with similarity already exists, if not filters the train dataset via sentiment and similarity filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW16DpIJqH2I",
        "outputId": "f2b03420-c343-46d4-8281-5020c937e4bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[94m[INFO]\u001b[0m dataset_name :  tweet_qa\n",
            "\u001b[94m[INFO]\u001b[0m min_thold :  0.75\n",
            "\u001b[94m[INFO]\u001b[0m min_sentiment_thold :  -0.1\n",
            "\u001b[94m[INFO]\u001b[0m dir_path :  /content/WSQASA//\n",
            "\u001b[94m[INFO]\u001b[0m data_dir_path :  /content/WSQASA//data/\n",
            "\u001b[94m[INFO]\u001b[0m model_dir_path :  /content/WSQASA//models/\n",
            "\u001b[94m[INFO]\u001b[0m input_file_path :  /content/WSQASA//data/train_data.pkl\n",
            "\u001b[94m[INFO]\u001b[0m input_file_issues_path :  /content/WSQASA//data/LID_all_issues.csv\n",
            "\u001b[94m[INFO]\u001b[0m output_file_path :  /content/WSQASA//data/syntatic_train_with_similarity.pkl\n",
            "\u001b[94m[INFO]\u001b[0m model_name :  all-MiniLM-L6-v2\n",
            "Downloading: 100% 1.18k/1.18k [00:00<00:00, 1.02MB/s]\n",
            "Downloading: 100% 190/190 [00:00<00:00, 190kB/s]\n",
            "Downloading: 100% 10.6k/10.6k [00:00<00:00, 7.19MB/s]\n",
            "Downloading: 100% 612/612 [00:00<00:00, 565kB/s]\n",
            "Downloading: 100% 116/116 [00:00<00:00, 83.1kB/s]\n",
            "Downloading: 100% 39.3k/39.3k [00:00<00:00, 161kB/s]\n",
            "Downloading: 100% 90.9M/90.9M [00:01<00:00, 61.0MB/s]\n",
            "Downloading: 100% 53.0/53.0 [00:00<00:00, 37.6kB/s]\n",
            "Downloading: 100% 112/112 [00:00<00:00, 80.2kB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.85MB/s]\n",
            "Downloading: 100% 350/350 [00:00<00:00, 354kB/s]\n",
            "Downloading: 100% 13.2k/13.2k [00:00<00:00, 10.3MB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 39.1MB/s]\n",
            "Downloading: 100% 349/349 [00:00<00:00, 320kB/s]\n",
            "Batches: 100% 2001/2001 [00:34<00:00, 58.12it/s]\n",
            "Batches: 100% 879/879 [00:15<00:00, 56.59it/s]\n",
            "100% 21096/21096 [00:00<00:00, 62374.81it/s]\n",
            "\u001b[92mjob success\u001b[0m\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(os.environ['output_file_path_1']):\n",
        "  !python '2_similarity_search_answers_artificial_df.py'  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zQX27koVqIjh"
      },
      "source": [
        "## 2. Create the test set based on the similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMPlm46ZqOWe",
        "outputId": "e5582f8c-9ecc-4654-f488-18ba939c18ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[94m[INFO]\u001b[0m dataset_name :  tweet_qa\n",
            "\u001b[94m[INFO]\u001b[0m min_thold :  0.75\n",
            "\u001b[94m[INFO]\u001b[0m min_sentiment_thold :  -0.1\n",
            "\u001b[94m[INFO]\u001b[0m dir_path :  /content/WSQASA/\n",
            "\u001b[94m[INFO]\u001b[0m data_dir_path :  /content/WSQASA/data/\n",
            "\u001b[94m[INFO]\u001b[0m model_dir_path :  /content/WSQASA/models/\n",
            "\u001b[94m[INFO]\u001b[0m input_file_path :  /content/WSQASA/data/tweet_qa_validation_form.pkl\n",
            "\u001b[94m[INFO]\u001b[0m input_file_issues_path :  /content/WSQASA/data/LID_all_issues.csv\n",
            "\u001b[94m[INFO]\u001b[0m output_file_path :  /content/WSQASA/data/tweet_qa_test_with_similarity.pkl\n",
            "\u001b[94m[INFO]\u001b[0m model_name :  all-MiniLM-L6-v2\n",
            "Batches: 100% 2001/2001 [00:33<00:00, 59.95it/s]\n",
            "Batches: 100% 45/45 [00:00<00:00, 73.32it/s]\n",
            "100% 1177/1177 [00:00<00:00, 33215.78it/s]\n",
            "\u001b[92mjob success\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            " finished test similairity \n"
          ]
        }
      ],
      "source": [
        "!python '2_1_similarity_search_answer_test_df.py'\n",
        "print(\"\\n finished test similairity \")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CB0jIQUeqOxB"
      },
      "source": [
        "## 3 Train the Q&A model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCwQBbN_sV2B"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMoDToAXqSvi",
        "outputId": "8a5c6038-0d97-4e6c-9311-18df956e11c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u001b[92m[INFO]\u001b[0m dataset_name :  tweet_qa\n",
            " \u001b[92m[INFO]\u001b[0m domain :  negative\n",
            " \u001b[92m[INFO]\u001b[0m random_seed :  42\n",
            " \u001b[92m[INFO]\u001b[0m dir_path :  /content/WSQASA/\n",
            " \u001b[92m[INFO]\u001b[0m data_dir_path :  /content/WSQASA/data/\n",
            " \u001b[92m[INFO]\u001b[0m model_dir_path :  /content/WSQASA/models/\n",
            " \u001b[92m[INFO]\u001b[0m input_file_path :  /content/WSQASA/data/syntatic_train_with_similarity.pkl\n",
            " \u001b[92m[INFO]\u001b[0m output_model_path :  /content/WSQASA/models/not_tweet_qa_finetuned_negative_model\n",
            " \u001b[92m[INFO]\u001b[0m model_checkpoint :  deepset/bert-base-cased-squad2\n",
            " \u001b[92m[INFO]\u001b[0m batch_size :  16\n",
            " \u001b[92m[INFO]\u001b[0m max_length :  512\n",
            " \u001b[92m[INFO]\u001b[0m doc_stride :  128\n",
            " \u001b[92m[INFO]\u001b[0m learning_rate :  2e-05\n",
            " \u001b[92m[INFO]\u001b[0m num_train_epochs :  1\n",
            " \u001b[92m[INFO]\u001b[0m weight_decay :  0.01\n",
            "2023-04-18 03:29:50.590197: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-18 03:29:50.766403: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-04-18 03:29:51.718218: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-18 03:29:51.718370: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-18 03:29:51.718396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Downloading: 100% 152/152 [00:00<00:00, 106kB/s]\n",
            "Downloading: 100% 508/508 [00:00<00:00, 339kB/s]\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 435kB/s] \n",
            "Downloading: 100% 112/112 [00:00<00:00, 101kB/s]\n",
            "100% 4/4 [00:03<00:00,  1.05ba/s]\n",
            "Downloading: 100% 433M/433M [00:25<00:00, 17.0MB/s]\n",
            "[INFO] freezing: 0\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 4035\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 253\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4035\n",
            "  Batch size = 16\n",
            "{'eval_loss': 0.5926478505134583, 'eval_runtime': 150.2277, 'eval_samples_per_second': 26.859, 'eval_steps_per_second': 1.684, 'epoch': 1.0}\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 553.8599, 'train_samples_per_second': 7.285, 'train_steps_per_second': 0.457, 'train_loss': 1.0878178894284214, 'epoch': 1.0}\n",
            "Configuration saved in /content/WSQASA/models/not_tweet_qa_finetuned_negative_model/config.json\n",
            "Model weights saved in /content/WSQASA/models/not_tweet_qa_finetuned_negative_model/pytorch_model.bin\n",
            "tokenizer config file saved in /content/WSQASA/models/not_tweet_qa_finetuned_negative_model/tokenizer_config.json\n",
            "Special tokens file saved in /content/WSQASA/models/not_tweet_qa_finetuned_negative_model/special_tokens_map.json\n",
            "[INFO] training Step Completed !\n",
            "\n",
            " finished train with syntatic data\n"
          ]
        }
      ],
      "source": [
        "!python '3_training_the_qa_model_on_syntatic_data.py'  \n",
        "print(\"\\n finished train with syntatic data\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3WCXfhbCqT9o"
      },
      "source": [
        "## 4. Run finetuned model over the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QnQ50ROZsehx"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-lBuMo_qTaI",
        "outputId": "dd28e04b-9c82-4f46-e41e-1e06583185f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u001b[94m[INFO]\u001b[0m dataset_name :  tweet_qa\n",
            " \u001b[94m[INFO]\u001b[0m domain :  negative\n",
            " \u001b[94m[INFO]\u001b[0m dir_path :  /content/WSQASA/\n",
            " \u001b[94m[INFO]\u001b[0m data_dir_path :  /content/WSQASA/data/\n",
            " \u001b[94m[INFO]\u001b[0m model_dir_path :  /content/WSQASA/models/\n",
            " \u001b[94m[INFO]\u001b[0m input_file_path :  /content/WSQASA/data/tweet_qa_test_with_similarity.pkl\n",
            " \u001b[94m[INFO]\u001b[0m input_model_path :  /content/WSQASA/models/not_tweet_qa_finetuned_negative_model\n",
            " \u001b[94m[INFO]\u001b[0m output_file_path :  /content/WSQASA/data/tweet_qa_validation_negative_with_finetuned_predictions.pkl\n",
            " \u001b[94m[INFO]\u001b[0m model_tokenizer :  deepset/bert-base-cased-squad2\n",
            " \u001b[94m[INFO]\u001b[0m max_length :  512\n",
            " \u001b[94m[INFO]\u001b[0m doc_stride :  128\n",
            " \u001b[94m[INFO]\u001b[0m batch_size :  16\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "Moving 0 files to the new cache system\n",
            "0it [00:00, ?it/s]\n",
            "2023-04-18 03:40:24.286926: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-18 03:40:24.540281: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-04-18 03:40:27.266721: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-18 03:40:27.266880: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-18 03:40:27.266905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            " 83% 10/12 [00:02<00:00,  4.56it/s]/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "100% 12/12 [00:03<00:00,  3.90it/s]\n",
            "\u001b[92mjob success\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            " finished trained model run over with test data\n"
          ]
        }
      ],
      "source": [
        "!python '4_run_model_over_dataset_with_specific_domain.py'\n",
        "print(\"\\n finished trained model run over with test data\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MLG2LwW2qYyx"
      },
      "source": [
        "## 4.1. Run not finetuned model over the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVCsGR88qc7q",
        "outputId": "a49e592f-8057-4a36-9234-c8980bcaa74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u001b[92m[INFO]\u001b[0m dataset_name :  tweet_qa\n",
            " \u001b[92m[INFO]\u001b[0m domain :  negative\n",
            " \u001b[92m[INFO]\u001b[0m dir_path :  /content/WSQASA/\n",
            " \u001b[92m[INFO]\u001b[0m data_dir_path :  /content/WSQASA/data/\n",
            " \u001b[92m[INFO]\u001b[0m input_file_path :  /content/WSQASA/data/tweet_qa_test_with_similarity.pkl\n",
            " \u001b[92m[INFO]\u001b[0m output_file_path :  /content/WSQASA/data/tweet_qa_validation_negative_with_not_finetuned_predictions.pkl\n",
            " \u001b[92m[INFO]\u001b[0m model_name :  deepset/bert-base-cased-squad2\n",
            " \u001b[92m[INFO]\u001b[0m max_length :  512\n",
            " \u001b[92m[INFO]\u001b[0m doc_stride :  128\n",
            " \u001b[92m[INFO]\u001b[0m batch_size :  16\n",
            "2023-04-18 03:40:36.891540: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-18 03:40:37.075916: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-04-18 03:40:37.903028: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-18 03:40:37.903140: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-18 03:40:37.903160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            " 83% 10/12 [00:02<00:00,  3.44it/s]/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "100% 12/12 [00:03<00:00,  3.49it/s]\n",
            "\u001b[92mjob success\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            " finished not trained model run over with test data\n"
          ]
        }
      ],
      "source": [
        "!python '4_1_run_not_finetuned_model_over_dataset_with_specific_domain.py'\n",
        "print(\"\\n finished not trained model run over with test data\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EpxWlP6hqrri"
      },
      "source": [
        "## 5. Generate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "f_ZvXivF0xnM"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haaFa-V0qrM5",
        "outputId": "ba6e7f74-a643-44b2-8517-6fb0565912cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u001b[94m[INFO]\u001b[0m dataset_name :  tweet_qa\n",
            " \u001b[94m[INFO]\u001b[0m domain :  negative\n",
            " \u001b[94m[INFO]\u001b[0m dir_path :  /content/WSQASA/\n",
            " \u001b[94m[INFO]\u001b[0m data_dir_path :  /content/WSQASA/data/\n",
            " \u001b[94m[INFO]\u001b[0m input_file_path_not_finetuned :  /content/WSQASA/data/tweet_qa_validation_negative_with_not_finetuned_predictions.pkl\n",
            " \u001b[94m[INFO]\u001b[0m input_file_path_finetuned :  /content/WSQASA/data/tweet_qa_validation_negative_with_finetuned_predictions.pkl\n",
            " \u001b[94m[INFO]\u001b[0m output_file_path :  /content/WSQASA/data/tweet_qa_metrics.csv\n",
            "2023-04-18 03:40:52.434225: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-18 03:40:52.623606: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-04-18 03:40:53.464280: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-18 03:40:53.464435: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-18 03:40:53.464457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Downloading builder script: 100% 4.53k/4.53k [00:00<00:00, 3.62MB/s]\n",
            "Downloading extra modules: 100% 3.32k/3.32k [00:00<00:00, 3.06MB/s]\n",
            "Downloading builder script: 100% 6.81k/6.81k [00:00<00:00, 5.48MB/s]\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "Downloading builder script: 100% 5.94k/5.94k [00:00<00:00, 3.50MB/s]\n",
            "Downloading extra modules: 4.07kB [00:00, 1.88MB/s]       \n",
            "Downloading extra modules: 100% 3.34k/3.34k [00:00<00:00, 1.98MB/s]\n",
            "total data on test :  186\n",
            "\u001b[92mjob success\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            " finished not trained model run over with test data\n"
          ]
        }
      ],
      "source": [
        "!python '5_compare_performance_of_models.py'\n",
        "print(\"\\n finished not trained model run over with test data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "ue7SKIrtNHk2",
        "outputId": "5183c814-e72b-49f3-c49b-9ce73b230143"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-24c9227e-c3d5-4833-b277-c29875545299\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>model_name</th>\n",
              "      <th>exact_match</th>\n",
              "      <th>f1</th>\n",
              "      <th>bleu_1_ngram</th>\n",
              "      <th>bleu_2_ngram</th>\n",
              "      <th>bleu_3_ngram</th>\n",
              "      <th>bleu_4_ngram</th>\n",
              "      <th>meteor</th>\n",
              "      <th>total_test_data</th>\n",
              "      <th>total_train_data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tweet_qa</td>\n",
              "      <td>not finetuned</td>\n",
              "      <td>0.510753</td>\n",
              "      <td>0.694614</td>\n",
              "      <td>0.417197</td>\n",
              "      <td>0.342114</td>\n",
              "      <td>0.279771</td>\n",
              "      <td>0.224116</td>\n",
              "      <td>0.576341</td>\n",
              "      <td>186</td>\n",
              "      <td>3257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tweet_qa</td>\n",
              "      <td>finetuned</td>\n",
              "      <td>0.650538</td>\n",
              "      <td>0.795524</td>\n",
              "      <td>0.577528</td>\n",
              "      <td>0.501968</td>\n",
              "      <td>0.437936</td>\n",
              "      <td>0.377177</td>\n",
              "      <td>0.600162</td>\n",
              "      <td>186</td>\n",
              "      <td>3257</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24c9227e-c3d5-4833-b277-c29875545299')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24c9227e-c3d5-4833-b277-c29875545299 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24c9227e-c3d5-4833-b277-c29875545299');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    dataset     model_name  exact_match        f1  bleu_1_ngram  bleu_2_ngram  \\\n",
              "0  tweet_qa  not finetuned     0.510753  0.694614      0.417197      0.342114   \n",
              "1  tweet_qa      finetuned     0.650538  0.795524      0.577528      0.501968   \n",
              "\n",
              "   bleu_3_ngram  bleu_4_ngram    meteor  total_test_data  total_train_data  \n",
              "0      0.279771      0.224116  0.576341              186              3257  \n",
              "1      0.437936      0.377177  0.600162              186              3257  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "metrics = pd.read_csv(os.environ['output_file_path_5'])\n",
        "metrics['total_train_data'] = len(pd.read_pickle(output_file_path_1)['dataset']!= dataset_name)\n",
        "metrics.to_csv(os.environ['output_file_path_5'])\n",
        "metrics"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
