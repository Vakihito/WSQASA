# Weak supervision for sentiment analisys (WSQASA)

We created this repository for the ICANN conference; here, you will find the scripts we used to develop the WSQASA method and a sample notebook of the fine-tuned model.

- Victor A. K. Tomita (ICMC/USP) | akihito012@usp.br (corresponding author)
- Ricardo M. Marcacini (ICMC/USP) | ricardo.marcacini@icmc.usp.br

## Astract
---

The growth of social networks, e-commerce, and journalistic media has resulted in the proliferation of opinions on various topics. Companies and government agencies are interested in understanding their customers' opinions about their products and services. Automatic sentiment analysis methods can be used to extract the general sentiment about a product. However, traditional sentiment analysis methods are inflexible in dealing with human queries, which tend to ask questions. Therefore, question-and-answer (QA) systems for sentiment analysis offer a promising alternative. This paper proposes a new method called Weak Supervision for Question and Answering Sentiment Analysis (WSQASA) that fine-tunes and extracts sentiment through QA models in an unsupervised manner. We investigate question-generation models associated with sentiment filters for weak supervision, generating domain-specific question-and-answer pairs for fine-tuning the QA model. Our method enables the generation of domain-specific question-and-answer pairs for fine-tuning the QA model, which significantly enhances the QA-based sentiment analysis results, even without the usage of labeled data.

## WSQASA - pipeline
![Proposal](/images/WSQASA_pipe.png)

<p>The illustrative figure of the pipeline of the WSQASA method on the left side shows the weak supervision process composed of a set of datasets, from which we split for the establishment of a synthetic database generated by a QG model. At the center is a filtering process of questions and answers, which is carried out through a sentiment model and a correlation between the synthetic answers and a dataset of issues. Finally, the last block defines the process of tuning the QA model, comparing it to the same untrained model on the test database.</p>

## Results
![Proposal](/images/f1_comp.png)

<p>Comparison chart of the different approaches to the WSQASA, the radius of the bubbles represents the size of the test dataset, while the axis <i>y</i> presents the relative gain of <i>F1</i>, the axis <i>x</i> represents the datasets under analysis and the colors of the bubbles represent the experiments carried out.</p>

## Files 

```bash
├── data
│   ├── tweet_qa_train_form.pkl         - tweet qa data
│   └── tweet_qa_validation_form.pkl    - tweet qa validation data
├── images
│   ├── f1_comp.png                             
│   ├── Paper_Conferencia_Victor_Akihito___ICANN_2023.pdf
│   └── WSQASA_pipe.png
├── notebooks
│   └── WSQASA_tweet_qa.ipynb           - notebook to call the scripts ( from 2* to 5)
├── README.md
└── scripts
    ├── 1_generate_artificial_dataset_.py   - generates the artificial dataset
    ├── 2_1_similarity_search_answer_test_df.py - filter the train data
    ├── 2_similarity_search_answers_artificial_df.py - filter the test data
    ├── 3_training_the_qa_model_on_syntatic_data.py - finetune the model
    ├── 4_1_run_not_finetuned_model_over_dataset_with_specific_domain.py - run not finetuned over validation data
    ├── 4_run_model_over_dataset_with_specific_domain.py - run finetuned over validation data
    ├── 5_compare_performance_of_models.py - check models performance 
    └── pipe.zip
```